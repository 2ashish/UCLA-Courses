{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is the softmax workbook for ECE C147/C247 Assignment #2\n",
    "\n",
    "Please follow the notebook linearly to implement a softmax classifier.\n",
    "\n",
    "Please print out the workbook entirely when completed.\n",
    "\n",
    "The goal of this workbook is to give you experience with training a softmax classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from utils.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3073)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3073)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3073)\n",
      "Test labels shape:  (1000,)\n",
      "dev data shape:  (500, 3073)\n",
      "dev labels shape:  (500,)\n"
     ]
    }
   ],
   "source": [
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the linear classifier. These are the same steps as we used for the\n",
    "    SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = './cifar-10-batches-py' # You need to update this line\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "    \n",
    "    # subsample the data\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = list(range(num_training))\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = list(range(num_test))\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "    X_dev = X_train[mask]\n",
    "    y_dev = y_train[mask]\n",
    "    \n",
    "    # Preprocessing: reshape the image data into rows\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "    X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "    X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "    \n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis = 0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "    X_dev -= mean_image\n",
    "    \n",
    "    # add bias dimension and transform into columns\n",
    "    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "    X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "    X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('dev data shape: ', X_dev.shape)\n",
    "print('dev labels shape: ', y_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a softmax classifier.\n",
    "\n",
    "The following cells will take you through building a softmax classifier.  You will implement its loss function, then subsequently train it with gradient descent.  Finally, you will choose the learning rate of gradient descent to optimize its classification performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nndl import Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare an instance of the Softmax class.  \n",
    "# Weights are initialized to a random value.\n",
    "# Note, to keep people's first solutions consistent, we are going to use a random seed.\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "num_classes = len(np.unique(y_train))\n",
    "num_features = X_train.shape[1]\n",
    "\n",
    "softmax = Softmax(dims=[num_classes, num_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implement the loss function of the softmax using a for loop over\n",
    "#  the number of examples\n",
    "\n",
    "loss = softmax.loss(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3277607028048966\n"
     ]
    }
   ],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question: \n",
    "\n",
    "You'll notice the loss returned by the softmax is about 2.3 (if implemented correctly).  Why does this make sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer:\n",
    "\n",
    "Beacuse the weights are randomly initialized and are not yet trained, so their output will predict randomly one of the 10 classes, so the probability for guessing correct class will be 0.1,\n",
    "Softmax loss for this should be -ln(0.1) = ln(10) $\\approx$ 2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: -3.647501 analytic: -3.647501, relative error: 5.572790e-09\n",
      "numerical: 0.451700 analytic: 0.451700, relative error: 2.844425e-08\n",
      "numerical: -3.732429 analytic: -3.732429, relative error: 1.804868e-08\n",
      "numerical: 0.647963 analytic: 0.647963, relative error: 6.568576e-08\n",
      "numerical: -1.820191 analytic: -1.820191, relative error: 2.507468e-08\n",
      "numerical: 1.109531 analytic: 1.109531, relative error: 1.441653e-09\n",
      "numerical: -5.199350 analytic: -5.199350, relative error: 5.170286e-09\n",
      "numerical: 0.594704 analytic: 0.594704, relative error: 4.507831e-09\n",
      "numerical: 1.727080 analytic: 1.727080, relative error: 1.661232e-08\n",
      "numerical: 1.027455 analytic: 1.027455, relative error: 7.851578e-08\n"
     ]
    }
   ],
   "source": [
    "## Calculate the gradient of the softmax loss in the Softmax class.\n",
    "# For convenience, we'll write one function that computes the loss\n",
    "#   and gradient together, softmax.loss_and_grad(X, y)\n",
    "# You may copy and paste your loss code from softmax.loss() here, and then\n",
    "#   use the appropriate intermediate values to calculate the gradient.\n",
    "\n",
    "loss, grad = softmax.loss_and_grad(X_dev,y_dev)\n",
    "\n",
    "# Compare your gradient to a gradient check we wrote. \n",
    "# You should see relative gradient errors on the order of 1e-07 or less if you implemented the gradient correctly.\n",
    "softmax.grad_check_sparse(X_dev, y_dev, grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A vectorized version of Softmax\n",
    "\n",
    "To speed things up, we will vectorize the loss and gradient calculations.  This will be helpful for stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal loss / grad_norm: 2.321937839839383 / 355.5657032496941 computed in 0.04790449142456055s\n",
      "Vectorized loss / grad: 2.321937839839384 / 355.5657032496941 computed in 0.002991199493408203s\n",
      "difference in loss / grad: -8.881784197001252e-16 /3.2759122096389006e-13 \n"
     ]
    }
   ],
   "source": [
    "## Implement softmax.fast_loss_and_grad which calculates the loss and gradient\n",
    "#    WITHOUT using any for loops.  \n",
    "\n",
    "# Standard loss and gradient\n",
    "tic = time.time()\n",
    "loss, grad = softmax.loss_and_grad(X_dev, y_dev)\n",
    "toc = time.time()\n",
    "print('Normal loss / grad_norm: {} / {} computed in {}s'.format(loss, np.linalg.norm(grad, 'fro'), toc - tic))\n",
    "\n",
    "tic = time.time()\n",
    "loss_vectorized, grad_vectorized = softmax.fast_loss_and_grad(X_dev, y_dev)\n",
    "toc = time.time()\n",
    "print('Vectorized loss / grad: {} / {} computed in {}s'.format(loss_vectorized, np.linalg.norm(grad_vectorized, 'fro'), toc - tic))\n",
    "\n",
    "# The losses should match but your vectorized implementation should be much faster.\n",
    "print('difference in loss / grad: {} /{} '.format(loss - loss_vectorized, np.linalg.norm(grad - grad_vectorized)))\n",
    "\n",
    "# You should notice a speedup with the same output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic gradient descent\n",
    "\n",
    "We now implement stochastic gradient descent.  This uses the same principles of gradient descent we discussed in class, however, it calculates the gradient by only using examples from a subset of the training set (so each gradient calculation is faster)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question:\n",
    "\n",
    "How should the softmax gradient descent training step differ from the svm training step, if at all?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer:\n",
    "\n",
    "Both uses gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 1500: loss 2.340083207460104\n",
      "iteration 100 / 1500: loss 2.0796101858228555\n",
      "iteration 200 / 1500: loss 1.9509828239589024\n",
      "iteration 300 / 1500: loss 1.9094554741811902\n",
      "iteration 400 / 1500: loss 1.9249155589928528\n",
      "iteration 500 / 1500: loss 1.8564142779293118\n",
      "iteration 600 / 1500: loss 1.955236276552195\n",
      "iteration 700 / 1500: loss 1.8739863809177661\n",
      "iteration 800 / 1500: loss 1.8282954997114702\n",
      "iteration 900 / 1500: loss 1.8613853134130016\n",
      "iteration 1000 / 1500: loss 1.801377358060681\n",
      "iteration 1100 / 1500: loss 1.838523870326722\n",
      "iteration 1200 / 1500: loss 1.93128662446051\n",
      "iteration 1300 / 1500: loss 1.8633359921607544\n",
      "iteration 1400 / 1500: loss 1.7841174341843271\n",
      "That took 5.049999237060547s\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAABEtElEQVR4nO2deZgU5fHHvzV7wy7LtZwLLLccglwKgoqKiiKeifE2RiUmJt4aPKImHsHEH0mMiQSPGO8LNQqieCCIci3IDQJyX7Lcy7Xs7tTvjz6mp6d7untmemdmpz7Ps8/OdL/9ds311vtW1VtFzAxBEAQhcwkkWwBBEAQhuYgiEARByHBEEQiCIGQ4oggEQRAyHFEEgiAIGU52sgXwSvPmzbmsrCzZYgiCIKQVCxYs2MXMJVbn0k4RlJWVoby8PNliCIIgpBVEtNHunJiGBEEQMhxRBIIgCBmOKAJBEIQMRxSBIAhChiOKQBAEIcMRRSAIgpDhiCIQBEHIcDJKEcxYXYFNuw8nWwxBEISUIu02lMXDdS/OAwBsGDcqyZIIgiCkDhm1ItCYtWZXskUQBEFIGTJSEew+VJVsEQRBEFKGjFQERJRsEQRBEFKGjFEE2/cf0R8HRA8IgiDoZIwimLNut/44ICsCQRAEnYxRBFmB0EuVFYEgCEKIjFEE2YbRX3wEgiAIITJSEWSJIhAEQdDJHEWQFRr8AxnzqgVBEJzJmCEx2zD6vzZnUxIlEQRBSC0yRhH0bddYf/zFqp0IBjl5wgiCIKQQGaMIigty8MEtQ/XnNaIIBEEQAGSQIgCAvOzQy62uDSZREkEQhNTBN0VARO2IaDoRrSSi5UR0m0WbC4loCREtIqJyIhrmlzwAkGWIHKqplRWBIAgC4G8a6hoAdzHzQiIqArCAiD5j5hWGNl8A+JCZmYj6AHgbwHF+CWQMGq0OyopAEAQB8HFFwMzbmXmh+rgSwEoAbU1tDjKzNjVvCMDXaXq1YRUgpiFBEASFOvEREFEZgH4A5lqcu5iIVgGYAuAXNtePUU1H5RUVFTHLUWNYBXy9WmoSCIIgAHWgCIioEMAkALcz8wHzeWZ+n5mPA3ARgEet+mDmicw8kJkHlpSUxC6LwTh076QlqJFVgSAIgr+KgIhyoCiB15j5vWhtmXkmgM5E1NwveXq3bRT2vMsDU7Hmx0q/bicIgpAW+Bk1RABeALCSmcfbtOmitgMR9QeQC2C3VdsEyRRx7KvvYzc1CYIg1Af8jBoaCuAaAEuJaJF67H4A7QGAmScAuBTAtURUDeAIgJ8ZnMd1QpbkpBYEIcPxTREw8yyER2xatXkSwJN+yeAGUQSCIGQ6GbWz2ApRBIIgZDqiCEQRCIKQ4YgiEEUgCEKGI4pAqpUJgpDhZLwiMFYuEwRByEQyXhGIaUgQhEwn4xRBQU5W2HMxDQmCkOlknCIof3AEXvz5QP35pIVbUVFZlUSJBEEQkkvGKYKGedlo3CBXf/75yh/x69cWJFEiQRCE5JJxigAAcgLhL3v3wWNJkkQQBCH5ZKYiyA73C+RmZ+TbIAiCACBDFUG2KVIoz+RAFgRByCQyUhGY85su3rxPSlcKgpCxZKQisGLj7sPJFkEQBCEpZKQiCFhsItt7WBzGgiBkJhmpCDo1b4hfDO0YdqzyaHWSpBEEQUguGakIiAi3nN457FhNbZ0WRhMEQUgZMlIRAJE5hjbvPYJdB2WHsSAImUfGKoK87PCQ0Ucnr8DAxz5PkjSCIAjJI4MVQca+dEEQhDB8Gw2JqB0RTSeilUS0nIhus2hzFREtUf++JaK+fsljxipySBAEIRPJ9rHvGgB3MfNCIioCsICIPmPmFYY26wGcxsx7iehcABMBnOSjTIIgCIIJ31YEzLydmReqjysBrATQ1tTmW2beqz6dA6DUL3ncsmzr/mSLIAiCUKfUiaGciMoA9AMwN0qzGwBMtbl+DBGVE1F5RUWFDxKG+HLVTl/7FwRBSDV8VwREVAhgEoDbmfmATZvToSiC31mdZ+aJzDyQmQeWlJQkTLbmhbkRxzo0a5Cw/gVBENIBXxUBEeVAUQKvMfN7Nm36AHgewIXMvNtPecx8cMvQiGMBKV0pCEKG4WfUEAF4AcBKZh5v06Y9gPcAXMPMq/2SxY7SJpGz/5pgEP3+OA2vztlY1+IIgiAkBT9XBEMBXAPgDCJapP6dR0Q3E9HNapuHADQD8C/1fLmP8rjixwNV2Hu4Gk98vBIAUBtk/OGj5di670iSJRMEQfAH38JHmXkWgKh2Fma+EcCNfskQC+OmrgIAtCrOBwB8t2kv/vPNBizfdgBv/3JIMkUTBEHwBdlea0OD3PAUFDVSuEYQhHqKKAIb8tVcRJrvWHKTCoJQXxFFYEN2lqIBSNUE5vKWgiAI9QVRBDbMWbcHQMjJwaIJBEGop2S8IujYvGHU89q+AlEDgiDUVzJeEUy97RQ8fnFvy3OVR6t1H0FQVgSCINRTMl4R5OdkYViX5pbnHp+yUl8RBCVoSBCEekrGKwJAUQZWbNl7BP+euQ6AmIYEQai/+FmPIG3IzbLWh7PW7tIfi7NYEIT6iqwIABTlZyPboWKZ6AFBEOoroggAZGcFsPaJ86K2EWexIAj1FVEELlmz8yA27zmcbDEEQRASjigCD9z25ndRz//k2W9x5XNz6kgaQRCExCDO4gRSvnGvcyNBEIQUQ1YEHsiycCgfqwli+TYpeC8IQvoiisADVmUsH/loOUY9PQtb9or/QBCE9EQUgYFJvzoZn9x+iu15qxXBgg2KOajyaI1vcgmCIPiJ+AgMDOjQJOp5oyKYs2431u86hO9/rIw4JwiCkE6IIvBAgAhfr6nAnz5ehRXbD0ScEwRBSEdEEXhgxuoKzFhdYXlOFgSCIKQrvvkIiKgdEU0nopVEtJyIbrNocxwRzSaiKiK62y9Z6gJysSJYu/Mg5q7bXQfSCIIguMfPFUENgLuYeSERFQFYQESfMfMKQ5s9AG4FcJGPctQJblJQjBg/AwCwYdwov8URBEFwjW8rAmbezswL1ceVAFYCaGtqs5OZ5wOo9kuOukKykwqCkK44KgIiaklELxDRVPV5TyK6wctNiKgMQD8Ac2MRkojGEFE5EZVXVFjb6JNNUPSAIAhpipsVwUsAPgXQRn2+GsDtbm9ARIUAJgG4nZkPOLW3gpknMvNAZh5YUlISSxe+42Qa2n8k9kVP2dgpKBs7BYePhfYqrKs4iGtemIsjx2pj7lcQBAFwpwiaM/PbAIIAwMw1AFyNPkSUA0UJvMbM78UsZRpQ67AkqKqOf8DeVXlMf/zo5BX4es0ufPvDrihXCIIgOONGERwiomZQqzUS0WAAjsl1SAmjeQHASmYeH5eUacC4qat8v4cxMEmLUhLXhCAI8eImauhOAB8C6ExE3wAoAfATF9cNBXANgKVEtEg9dj+A9gDAzBOIqBWAcgCNAASJ6HYAPWM1ISWaj34zDKOfmeWq7ddrQjPzrfuO4KfPfovc7AC+uud0AImveazpBNEDgiDEi6MiUMM/TwPQHcr48z0zOxq8mXkWQuOVXZsdAEpdylrnHF9aHNN1Q8d9GXEsETP38BWB1q+oAkEQ4sNRERDRtaZD/YkIzPyyTzLVK5Zu2Y/Rz8zC6zedFHdf4ZvWVNNQ3L0KgpDpuPERDDL8nQLgEQAX+ChTvUIzLX2xcqd+bP2uQ5gw4wcctXEgT16yDSP/NjNiti9ZLARB8AM3pqHfGp8TUTGAV3yTKMW4bGAp3i7fEnc/xvDSyyfOxo8HqrDvcDXGnntcRNvb31yEmiCjJsjIybIe/kOmobhFEwQhw4llZ/FhAF0TLUiqMu6SPvj+sZH684EOqartMA7Yh9XY/wNHo7taog3yIfUgmkAQhPhw4yP4CKHRJgCgJ4C3/RQq2TTKz0ZxgxwAQCBAyAtk6ecmXDMAAx/73HOfa3ZW6o/1iB+HMdy8Sc34XFYEgiAkCjfho08ZHtcA2MjM8dtKUphFD52d8D6/WRvKOhraA6CM4keO1WLaih24oG8bEJGudc2DvPE5xeEsnr9hD346YTbm3n8mWjbKj6EHQRDqE258BDPqQpBUIhCluEBhXvwJW7XZvDbD/9PUlXh59kaUFOXh5M7N9XbmFUGYIojDc/zStxsAAPPW78Hovm2iNxYEod5jO6oRUSWsJ5wEgJm5kW9SpTD5OVno3rJIL1EZC9oYrmWleHn2RgDA3kPhPoMIRWDxccRjGhKrkiAIQBRFwMxFdSlIOhFvVUq79BDVtcGw50EGZv8QMikdrKpBRWUVSoryQj4CGc4FQYgT13YOImoBQDcoM/MmXyRKA9wUoYmG5huoCYYP/MdMioCZccVzc/TnFzzzDWqDjA3jRoV8BKIHBEGIEzf1CC4gojUA1gOYAWADgKk+y5XSxFt7QLv+f4u24YuVP+rHrVYERsIynKorgh8PHEXZ2Cl4Y557vZysjWnHaoKOWVoFQah73OwjeBTAYACrmbkjgDMBfOOrVClOMM7BzLhj+HPDjuMH3l+GyqPV+vloKw9tMF+/6xAA4L2FqR/I1e3Bqbj6+fDaRMdqglgdh79FEIT4caMIqpl5N4AAEQWYeTqAE/wVK7Wpjdc0ZHhs3jn8zdpQFtOoioDCw0cpTRJQzF63O+z541NW4Oy/zsSWvYddXV9dG8QzX66xTc8hCIJ33PgI9qlVxmYCeI2IdkLZT5CxxOsjqDwaevsCJs9zbnZIN7vZWayvLtJDD0SwcNM+AMCeQ8dQ2qSBY/u35m/GU9NW42h1EHef091n6QQhM3CzIrgQSlqJOwB8AuAHAKP9FCrVMfl440KL6dfIDoQ+EjuF89mKH/Hh4m1hspj1wMbdh/B/076PmqY6FVJYa1s23PoOjtUoL/hgVUbPRQQhobhRBGMAtGHmGmb+LzM/rZqKMoqTOzfD/ecpCeL8HEBzskIfyZ5Dxyzb/Oeb9fpjTVnMXb8nrM1NL5fjH1+uxaY99iaXvYeOxVTq8u+fr0HZ2Clx+0qA0OY9t6usgGkzniAI8eNGETQC8CkRfU1EtxBRS7+FSkVev2kwxpzaGUDIRzD5t8MSfp+crFCKiVFPO1dHM47FRgVVU6uFqNoPmI98tAJXPjfX9rwd//hyjWPfbslSTWO1LldZWQGtvSgCQUgUjoqAmf/AzL0A3AKgDYAZROQ961o9onVxAQCgVXE+5t5/Jo5vG1slMyu8jm/Gwb9aHfyZGevUaCJNIRghk18imSYizysCj+0FQXDGSxrqnQB2ANgNoIU/4qQHz107EH/72QloXpiHlo3ykW1TMyAWzJvMnDAOh1U1SiTNboNJyU1/mvI54Y/T8KtXF+jHfzxwFE9+ssrWBGTc1RwMckwKxaupR1tBJNJPI6QWOw8c1X1BQt3gZkPZr4joKwBfAGgO4CZm7uO3YKlMSVEeLurX1pe+vQ5wxsFX+/EYB1U3JpTdh6oAAPsOV2Pqsh368TvfXoRnv/oBCzbtDWtvToFdXRtEp/s/xp8//d6b8AhFTbl93Vr7eEN4hfgZO2kJ7nlncUL7PFYTxIlPfIF7301sv0J03KwIOgC4nZl7MfPDzLzCTcdE1I6IphPRSiJaTkS3WbQhInqaiNYS0RIi6u/1BaQi5/ZuFfO189bv9pQ2wjjOV1nMoqotTENmTnz8C8zfsCfiuK5Y7FYEHH7fl00RUG7Qbf5eTUPiI0g6b87fjHcWJHYjo7aC/XT5jw4thUTixkcwlpkXxdB3DYC7mLkHlJ3JtxBRT1Obc6FUO+sKJTrp2Rjuk3LEM1l9+su1nvrXwkgBYIPqFzDai9w6VY2RSBpONQ8SYafXVwQSNSQYqC/JFOes2433v0v9Xf+xlKp0BTNvZ+aF6uNKACsBmO0pFwJ4mRXmAGhMRK39kskvurUIT9RakJtl0zIx2A2CVz4/N8JOb85fZMfHS3dEHnSoghYqoMNhz63YWXnU8rg+sEdRWFv3HUGN+jpCiiPKzYS0JV12yLvl8olzcMdbqW/m8k0RGCGiMgD9AJhjFdsC2Gx4vgWRyiLl+cOFvfTHd57VDY+M7hWltXeyTYVyos2GzQOkVYjnR4ZVRDS0u27ddwQzVlcYjofP4t0Myic+/oXlcadw0F0HqzB03Jd44uNVAEKmIfERCELicOMsbkhEAfVxNzUbaY7bG6jpKSZB8TMcMJ+2uCTiF05EY4ionIjKKyoqLC5JLvk5oRXArWd21esdOzGgQxNX7bIiFIF92873f4ylW/frz48cq8XvP1iGDxdvw/Tvd+Lbte43kGlO4bvfWYzrXpyHg1U1WGuovayNxfGEnzqZhvYdViKgZqxWkvOFooZEEdRH6otJKN1wsyKYCSCfiNpCiRy6HsBLbjpXFcYkAK8x83sWTbYAaGd4XgogYrrKzBOZeSAzDywpKXFz6zrn/vOOQ5/S0H6Cp6/oh8Gdmka0e+yi3vrjG4d1dNW3eUWwYONem5YKswyD/bsLNuOVORtx6xvf4fr/zMeVz7vfQGZepl/7wlyMGD9Tf84eVgR2ZAW8mXq0jdea4qiorMJTn34viqGeIAu95OBGERAzHwZwCYB/MPPFAMxO38iLlF1LLwBYyczjbZp9COBaNXpoMID9zLzdpewpxZhTO+PD34R2Gl/Qtw26tYws8vaTAaX6Y7ffefOKwAmjX+BIHFk6zZXYtARx0B22yn87s853m/bin9PXYuu+I7b30MNBXQ7kZNqJPHbSEjwzfS3mrE9O1pMtew/r/gshfkJ+p6SKkXG4yT5KRDQEwFUAbvBw3VAA1wBYSkSL1GP3A2gPAMw8AcDHAM4DsBZKYrvrXUueBliZO8yzezccOOotwZpxM86KbWZrXAIwmYTsTEMX/+tbAEqeJjucdgqbD2eZTEmHjtWEyVSX7Nh/FMOenI4xp3bC/ef1qHsB6iGpkAgxE3EzoN8O4D4A7zPzciLqBGC600XMPAsOyZFZ+dRvcSFDWmL1nfY6u48F436CvYerY+7Hrjaz2UlsnMzPWrML3VsVoaQoz9U9tE3ZdiuCs/46U5VFaWh2LmuXmdNmJJJPlu3A/A178PvzwxfCuw4qG/FmrfGeuC8d2X2wCs0K3X2usSJqwJrZP+zGd5v34tfDu/jSv5t9BDOY+QJmflJ1Gu9i5lt9kaaeYfWl9nPA0qiqToypwi6UL6iHiyr/a/WVAXD1C3Nx+cTZru8RsPERVNXUYueByJBT7Z66MlIv9FPB3vzqArwwK3KfRSZRvmEPBjz2OaYs8ddya7cg2LrvCA5lcOrxK56bgz9/4n3nvlvcRA29TkSNiKghgBUAvieie3yTqB5x11ndop73axXsdu+AE/YrAuW/Jn8wGD44/1BxyNSP/SAdsIkC+vWrC3HiE5Ehp/o9TeU8/dADCzbudbUZqA50e9JZpkaizfXbF8Nh/3SGjvsSl0+c4++9Mxg3zuKeatjnRVBs+u2h2P4FB5oV5mHRQ2dh/gMjLM/7FSr3xaqdzo0SgFbLQBuc7dJSm8fJVTsO6GGs76opCsz7AsyvIVSRTflvNg0drQ7ixVnrE2pjvvTZbxO2GSjdS2vqpVE9vr1rdx5E2dgprsOWo/0mjGHRQmJxowhy1DDQiwD8j5mrIaY81zRukOvaXh6N+849LgHSuMe4X8COhz5YDiA0iBvt/NMNA7n5yzLyb1/jyufnhg0O2rW1DllMtTOhVYny4LEpK/DHySvw1ffh+0zmrd+D/y3a6vhaYsHtoLhp92Ec9/tP8Nb8Tb7IURfoiQY9/vTnqDWqJy91Z1Kyek9ve/M7T/cUvONGEfwbwAYADQHMJKIOAHwIRRGiUZfmhyc+XokR42fiaxdOUGZGrUXq0BctqqiZMe5p0Ab0zvd/jLuiZLRkk29AU0IVlYrj1px477J/z8Ztby6y7KuisgplY6dgqmGQ+nzFj1i+zdvM0+mz+aHiIACEZXZ1Yv/hat3Ed+Botafor7P/OgPPf73OdXsjB6tqcOBoZIBBrF+/gMeVhFWz/y1ytxNeiB03zuKnmbktM5+n5gTaCOD0OpCt3uNlmV2XOVgmznQ3iFRW1eC0v3wVtslMw7g6cGOuMVqV3ltoP4PXmoVWIcpzLWTWTXju2p0H8fHS7Vijrnpenr1RP3fjy+WuKsPFgvltqKkNYq9NOdK+f5yGX726EABwzQvzcN7TXwMAXpmzEU98vDLqfVb/eBCPTYneJlI2xvb9R9D/0c/Q55FpUdp56taQstzdhXq7OGwOK7YdwDvlm50bCjpunMXFRDReS/FARP8HZXUgCLY1kY2V0S591jmKqDbIuO7FebbniRSfhPYD11YE2sBxTNUIRMDhYzVYteMAysZOsexrxPgZ+PVrC5GrblN261wfN3WVq3ZWsgORY9sjHy1Hv0c/w5Fj1v6Dz1cqqZgXb96nH/v9B8tcK2ovvDJnI4b86Uv7gjDazB7A1c/PxS9fKXfVr9fpi/k9Omv8DI89AOc9/TXueXeJ5+syGTemoRcBVAK4TP07AOA/fgolRJJukSnVHivs1AY5LLGdFVc+Nxefr1R8D9oKQjM9aIrggfeXoedDn+Kdcudon2yPimDCjB9ctTOjOVpnrq4IG9S1UMzDx+zDIjcbFK1x9bBjv3U211iZ/UP0aCCjs37W2l2e6wW4Ng2Z2q3ZedDTfYTYcKMIOqsFadapf38A0MlvwTKB+uxx91pc3i7iSGP1j+EDgtZ/QP0GawPIDnXvwfb99mktzH0cc1G8J1Fc+M9v9MdkmGXbYUzP0e/Rz/THg//0Bd6av8nz+xwroYmI9f2qa4OWqwndR+Dy2y5J5yLZFiVFS6JwowiOEJGeRIeIhgLwX7IMoD5vp9/uccbqtdBMaP+A9VLJsr6CifcWqqGrMRZAdjto2bkt3BTZibYQ/N2kpZYFhfzAyUc1/C9foduDU8OOTf9+J+6dpJhooumrI8dqUTZ2Ct6ctynu2ZE52u2+95Zi2nL3TvpUpC6iptwogpsB/JOINhDRBgDPAPilr1JlGKP7tsGGcaOitqmLHcmJZLeafsEtf/FY71ibCcfzrrw2Vwnn3H+kGmVjp+CNec7hnZry3ll5FLer0UhOg6TdebJzHli1sSHRJiInou38NfPo5FBV22h6XkvV8cz0taGkczFqhMWbwyO+3pi3CWNeWeCpjw8Xb8OkBJfgjIe6WLG6iRpazMx9AfQB0IeZ+wE4w3fJMph7R3aPOJZeasD/CmJBZvzm9YVYvMV9qOf+w9UY+bfICKcfDygD0ZvznSNNZq3dhemrdmLc1FVYt+uQY/toaCsC82Y640rRSf9XHq1JyGY1pwVZKPrHS6fGh26jhjz0H/2WMXPrG9/ZhjAzM37x0nx8vSY5dVH8siK4rlDGzAcMhWXu9EUaAQDQtnFB2HOi9HMW+02Qgcke894s2boPq3Y4b5SLxjUvzMP1L83HQoeaEEbsPjttpWCMsAoGOWx27fSxv1W+GUPHfelalljRncUehlq2fRLtmsQNdH4Mmkerg/hy1U7c+F93UVOJxq8JVqylKmVYSiDmN9Mu9bIQIhYnaVG+68J6juw7Etp0dehYDR75cLltGKitIrDwETz95RoMe3K647VGdhuiiewGv3fKN+u7fN2yYdchLFVXXLGsCIyyuL3Mbf9Hq2sto62M93QKQIiFmFZG8d7T8NirL80tsSqC+uvlrEPsi8KHnwgQ4ZxerVCQk4UrT2qPYV2a14F0qU0sPwjzNce3LQYAdGtZGNF24aboM36jk3pdxSG89O0GvPTtBsu2dj4CrQ/jgDXTIYTWCbu35Z53l3hO2jb8qa8w+pnYN9cZx2ErBfVO+WbsPxK+i1n3ETh8vGc89RV6PvRp1DbGXebz1u8JS3sSK3quqyQFetS5IiCiSiI6YPFXCaCNL9JkGFrq5Bw1nv0vP+kDAOjdpjiibZvGBVj56Eg8cfHxePXGk8LOTbl1GC7om1kfSbSqZ1YEgxwxuJhTYBuH60vUojp2WA3t0fYDmFm+bb/+GqKvbtytBvccOob9h6sjBorq2qArx72TSUZTZlat7MqEGvvcti/cqb1i2wHc8+4S3GOyxbs152yzcZIbr64y+E4u+/dsXP/SfFd9R8OcBr2u8eu2toVpmDmyzqKQEP55ZX80aZCDQR2b4pendsKvhncGAPx0YDtc1K+trhh0HMaCXm2K0bRhrk/SpiZefxA1FsnstIEilh+1VTSPndPWKnz0fUMajfkb9uDxKSvxwnUDLe7jTp7+6h6D1Y+dG3Z87KSlmLTQOQLG8S2IYhI5ZrMhz9h23oY9KBs7BfMeOBMtivL1Eqo7K6vC2jvJcbS6FhdEW6UYrjfnnQKAb9buQstG+ejSInIVaGb/4Wrk5QSQn5OlHzOnYK9r/FJAbiqUCQngDxf0QvtmDQAAo/q01o/fZypxGKEEYK0H2jYuwNZ9RzC4U1PXMjz10764O0pCt/pMkDnC0aYN3LH8tqwG6KMeCgIFDNrhgfeXAVDSLOt1oWPEOAtnZny0JDEJ26I5i+12Zlu9r5v3HEaLovyYgx++31EZsbkw7J4G+awU81VqokOncG1AyffUt7QY/zPUIk/23p86XxEIieW6k8tivra4INLJ+c3YM7D7YBUa5ikf4W1ndrW1UWv0b99Yf3zhCW0yKqtjTZAjZlPawK3vSfAwOFk1tR0Qra636OAXFqYLz7l6DDerDXLCojqi7Wewy09kNWiaJzrmFk4557x8RtqKIECxR9uYw5Pd9vPN2l04oV3j2G4ahVRzFgt1iN3u2WaFefqytYkL01CnktByONPikILMOHg03IZ/tEaZMcYSgWT1kdgNllY/XqvP1Kq+tNeNhMZ71QTZ9rsTMw6moaPVyi7h1+dushw0X5uzCSPGz9C/f8bcS0r37nwVtuIZLtcUVELLmLr4qmzecxhXPT9X31UdL8aPMNXCR4U0J912KsfLR4u34caXw2O/NQWgzeS/82CWsRpgLax6ChY/Xr/efeNAEWR2nEF/vHQ7ysZOwY+V0R3KWjfvfReZIry6JnTTqcuUvR1Pf7FGz/tk5K3yzVhrk0hu674jrje2uaFaz0gbeVGs9Y93H3J2vFeqE44ffEiYl/QNZV4hoheJaCcRLbM534SI3ieiJUQ0j4h6+yVLunNhv9gjgjRHtJmBZU1i7jMd0ezwRoImReAFu1xK75RvRo2pP/MsbvfBKvzrK3eZTL1OZr2uCLS0Gqu2Ry98E60b44pAK+1pDguNhlHmeIc54/XR9hHsPmhdByIah6pqcPOrzukqoq1qjlbX4vY3v8NOCyXphnRcEbwEYGSU8/cDWMTMfQBcC+DvPsqStiz7wzn43Tmxl6m866xulsevPLF9nZe/TDW0WPDqBOVyead8C+55d0mEr8ZsGhrw2Oeu+/RakCjMR1Dr7CNwW4s4miKwmqVqZjen+wLO+w28YLy8Jsrn6kVRAYoS6PXwp1Ed1WYZrFYiU5ZsxweLtuFPptoWB6tq8OQnqyz9LXWxocw3ZzEzzySisihNegL4k9p2FRGVEVFLZvaW6LyeU5gX30dkNyMkInRQo5gyFS3pqF34o1c05+QeU9WxeH68lRZlI6NhHEhrmR1tUNqKIx7bvNUs1VGxhLWN3A1spxCcTEPG11GjfsBWl1RWeXtfvSoOu/tq0plfx/hpq/HiN+vRoWkDzN+wFws37cX0u4dHXJ92isAFiwFcAmAWEZ0IoAOAUgARioCIxgAYAwDt27evSxnTirN6tsRJHZsiyIwnPlZmHNoXrlG+1Uftr5/g1jO74ukv1vh6j3jQVgS2VbkSQNnYKWhdnB/z9ca6zm74zuB8dRM1pE0U4qlrMG+9t9QVZoy3Hv2P6DuZoymkp79Yo2cyBaK/Jq/jqVPz73dUomWjPDRukKv3HbCwt2gDufl1VKkrqOogR933UR/DR8cB+DsRLQKwFMB3ACw9OMw8EcBEABg4cGBGpLcoKcrTC7K75blrQxuSQoqA8PjFvXFy58i0FH77ixvmZjk3SiJ+FXUxr8K81maIh+v/EwpBnbm6AgeOWjtFN+0+jBaN8vThKNpbwRwZemvk9/9bHouoxjvoj7RVVZBh+f23+84era7F+M9Whx2LJSzYVkKL1//+d1sw6vg2yM0O4Jy/zURZswaYeO1APS3Hsq0WfhdNScQoU71TBGom0+sBgBRj2nr1TwAw857TE5bP5KqTOlgeT3hooQmrzXH2bSlhtvpkM2vtLvxiWEcUWa7C6g67ur1VNbU49S/TMbpvG1fRY//66gfP9SKcMN7WzjL39y9WRxyzE9dqF3E05aWd2r7/CJo2zEVedvRJi1VXd7y1GOsqDuGus5W08Rt2H8bZf41Mc24lk5vfXjDIGPWPWVhpcOTXu30ERNSYiLTg9xsBzDSkuc54CnKz4vYPOGH3VWzgMJN3sysTUAZ397LUn3DWRZv3of+jn+HeFC2gvnWvkuNo0ea9tiGvxvj+d8qd6zTEg93gZnXY7ntiZd4L5ZCyvqamNoghf/oSd77lvNvebvzd5TH6SJfJJJJV91U1wTAloFyfZj4CInoDwHAAzYloC4CHAeQAADNPANADwMtEVAtgBYAb/JJFsCbLZqA+t3dr7DpY5VhM3omAl/Vv/dEDOh8uTs2d21oK7aYN82wHSWNt5Q27DydcBuN97Ux0VkftnNpW6SS0fo9YnAsy42dqNtbPVjjHp9jdN9ujjUfrx75GRQgri0DamYaY+QqH87MBdPXr/oIzVl/iOfediWaFuXhh1npLReAly6kX01O031O6mo1SVbdVq7PnnABZOjTrGtt07BbH7cpLezUNHT5WgwVacSEXH5SdD8XrruXff6DtZ9GKEgUxf8Ne/XzYPggLm1m9Mw0J/tKhWQOMdhi0rQretCrOR05WIGxgfvaq/vrjM3u0cC2Dl99INKXx5pjB7jtKIVJ187amVI9U19oOrH5jfG/2HbE2r1g5aM0zc61NlcW+BdfpvdVmny7fgbKxUyxb2w3AWQGKMN/Y8crsDbpC0X4b/565Dlc8Nwffrt0V0d5q8uPXhjJJOldPmXHP6Y5tNEdhkwY52Hu4GheeEFIcxqV7rLlavNj9oymCziXOKYMF92g7qZdvO4Dl25LjljOOq3e8tcixjd2xZ2f8gF8P7xLVR2Ddd+TJX0Ypcv9OuXVIZ3aAcO7fv7a/kQFjdJX2fd+yVzG7aea3MNOQxQtIOx+BkPpo43vXFkV4++YhYeeM43KseYm8KJCDUXK/pKsjWZE7tUxaizfvs52B1yXGmb2dw9XKLm8eB//8yff49fAu1oNmFE0QdsrF12vCDOuUIDFPkkjZLPjGvHBH/IMfhFKhWKU+8SvXkCiCDEZz5lo5pewGfy9KIdtD1JBGUV42Kk1KgdLUgJmscobRMDqBUx1LH4HNe2qlCDbuOWTb9zPT14Y9tzItucGrs1gjQITHJq+M2sYqV5JfX6k0/YkJiUD7Dlv9uMjhcV8XudZjmS1Zfc/93u/gF35tWMsU3lkQaY4xv6MlRXkIBlnP+Gnkn9PtE/uZ7fpPTIk+KNsR62q5Jhh0THNh7SyO6XaOiCLIYKIlHLMNb1OP/++WoY79xzJbsgoDTGQ6eSE1iHVma5609C1tjL9+vjoixbgXCMCqHZW256Nlp504c11M93x1zibHCY5VDiyJGhISjvZFjFY45dohHcKUQuvigrB2nZo3BAA0syiMEyDC3WdbZz81c75avrM+rQgEe2LdqWye+R+rDdo6cr0Q7TvW9YGptues9ihYYWXbn7xke9RrRj0dmXdJnMVCQhh/WV/9cVTTkMW55oV5GNAhVMdgw7hR2LznME7583QAwL+vGYC73l6sO36zs8gyvtvMdUM66Dlxxo48Dlv3HcHBqhq8q5oG6rMe6NKi0LZQS31mlkW4pBvM5TxnxrnpUcPv71iixm/xEQgJ4ZL+pbikfykAoHGBMos/rlWjiHba78L4xTu+bWQ7I+f0aoUlD5+NIZ2aAQCyAgFLRXBcqyIAQNvGyuqib7vGuj+hcYMcPHJBrzCzUn1eEXRrKaGxXvDD71ITZFd1BlIBMQ0JCad9swZ49+YheOwii+Jwmv8A0WdL5nOBAOGyQYqi6d6yKMLm/48r+qG/uqroVNIQ0+8ejov7tdVD/az8FlaK4O1fDok4lo40L8xLtggZT22Qw9JX+0Gihm9xFgu+MLCsKfJzIpPM6QVLGCguyAEAlDaJLGQTMCgMjYv7lWLDuFFoVZyPqurwFcH5fVrjTbU84tdrdqFj84YgIt2cpCXaM858rJzFJ3Zs6u4FpjjXnVyGNnHUKwCAwZ3qx3tRn2Fm/XcUD7IiEOoUbRMXM2NAh6aYcPUAPDCqR2Q7B6tNh+Yh5XFix6YgIstZTbNCxUxVUqTMkI1tYg3RO6dXy5iu85MJVw8Ie54TCOC6k8vi6tNLum8hOTCib3Bz3Y+sCIS6hAwrAgAY2buV5crBadfvL0/tjBbq4H71YOu6CADw4Kie+PvlJ+jO6N+e0QV9SotR/uCIGKRX0JRKvPxuZPy1nTuXNMQD5/XAyN6two4nwv0R6+5Woe5gVnwRw7pEFojy1o+sCIQ6pHebYgDAyV2aRW2nDWR2Q1FWgHQzjvlLbBwEG+Zl48IT2urPy5o3xIe/GRZmQ2/SwNvSOhFO5k4lDXH90LK4+7lmcAfcdGqnuPuxIt+hqIqQfF6ZsxFHqmvRKk4zoPgIhDrl+NJiLH7o7LDB2Qp95RC1TfiAXNpEiRbyYjOde/+Z+Oqe09GppCEuG1jq6ppEzJNPLGuakBm3VboAQHn/4tVXudnx/4xfv/GkuPsQ7Hl08goAwA8V8UUniY9AqHOKXczAY0kIN/6yEwAAjT0ogpaN8lFckIMv7xqOP/+kr/MFiN23YO4j1l56G8JtD1VZbzwioqgmM6dqcQCQlwBFkKZ5/dKOwzbfA7eIIhBSEjdjrXlPQsM8ZXBLh0w8RLGbmH42qL3+eFhXaxMbAWiQm41L+luvvNzEzSdiRSDUDTVxFoAQZ7GQtkTsNYiS4ygePrhlKNo1DaXASISPIBCH6cZ42YAOoRDPu86KTLvx+EXHW/bhNAPs0KyBZb2GS/u7M5/ppINWNhFv2G0yeNRqz44HZEUgpCRexkgtv7xmc7erAxsrLRvlYWjnUFSG7siOUx8kwsRkxGgK0rouyM3Sd2QbcVoRvDlmsOWK4FxTdJITdaEHcmJIS27HDcM64h9X9ndumGL0al0c1/XiLBbSFvPP37hZLR60FBUa5tKb2n2MK4PuLYsc++3SIjTD9iOlQRNDgj6jB6IwPzL1l3b7O0Yoq4jj24YPJHbvoVcH9/GlxWjsMSrLKw+N7oVTu5UkpK8erRshPyf9hq94a2ukXfgoEb1IRDuJaJnN+WIi+oiIFhPRciK63i9ZBP9w87U0p42Ilv7aCx/fegq+vOs0/XnANPhp99GcqVec2E5PbxEN44+txqJurFtc+U8MbYosFEFHNburLpvpHQ8yW9/H4+S7UX4OFj10treLPHL1Se2dG7nkgr5t0nL/RLwSp6OP4CUAI6OcvwXACmbuC2A4gP8joshcxkLaY3YWm2fusVLcIAedDPZxc7/as9zsABY9dBYevbA3Hh7dE2/cNDhqv8bfmrYimHC1P2YIo8Raeo1Tuirmra/vPR0dmkWm9Qi7Po0S8sUTgWVkwtX9kZsdSNj3qC6J9/NKOx8BM88EsCdaEwBFpLwzhWpb+8K1QkrizUeg4Fc2UbsVAQFo3CAX2VkB5OdkYUjnkC3eqsBOYV62vhtai/93YyH6xxX9wu/v5t0xNClQd26f3Lk5NowbhXZNG4SUqMXaKyeLIsxjOjGOF7lpkK5C+/6YP+90IF6R66OP4BkAPQBsA7AUwG3MbBlbRURjiKiciMorKhKTf1yoQyIylCr/E23vNJsKtKfRZmFmc8zdZ3dT6iqoBXVqdUUQXdbjWhWFKRjlvs4yG5WFZsIy1s+NZka7YViniD7iJdEOfDOJTKmRliuCOD+rtFsRuOAcAIsAtAFwAoBniMgy4T0zT2Tmgcw8sKQkMc4mITFoszPNrBENbeDXB7cEyxLpLA6tCMxos2/z6uSmUzuhdXGBnshNi/t2+v0FmWP6iRtvn6fKdNSQsTW0IohE26GdSPyyQXvBaXzXFUEargji1V1p5yx2wfUA3mOFtQDWA4g/u5dQpzRpmIvfn98Tr0VJUWCeBVkVvUkEAZtvs9WP77M7T8VL1w+KUATac00RaCuCs3u1xBUntrO9d5AjlUq037xVjibrFUH4dcb37MoT7Z2vfs/srUjUxjbz+zbQ5ODXFEC6mYbszHhealLUR9PQJgBnAgARtQTQHUBslaCFpHLDsI5o19TeqamFJRao6RJC+YkSbBoyjZqh/iMHjNImDTC8e4uIgVZ7GloRKH3kZWfhT5f0sbxvp+YN8eiFvcP6GqXWYHbCaLZqpKbcyA4b4OxjbaMNhLEq2Xg+ETemGjdDt9mUZw5rzYqy0ktlvhl7huWkZECHxgCUCDFzlJiZtIsaIqI3AMwG0J2IthDRDUR0MxHdrDZ5FMDJRLQUwBcAfsfMsRUyFVKae87pjodH98R5vZXBMZE2bSNZAcJtI7rqz0PhqvbXmGexpK8IlP9uwkc/vu0UDOncTH9dRfnZ+Kdhs5NVkjwy/QeAS/q1xR0juuH2EaGdx3ZJ/YyzyFQxlRtNNROvGeA6OaAZ88u586zutvdJN6y++9rrGVTWxDLVOwD8XK1ZkXY+Ama+gplbM3MOM5cy8wvMPIGZJ6jntzHz2cx8PDP3ZuZX/ZJFSC75OVm4fmhHfQZrrnWQKIgIrYsLcO0QZeeu/XoghDlhW+SKwDo3TI/WIXdWqEobh/Xh5nUaB/HsrABuG9EVDQ3+FqvNd/MfGIHpd4f2T1jBDDR0kbAu8rrYPxTj+Hx2r1bo265xRBu34ZM91ff38kHt0LNNuOsw1RVBtIpxZtGXPnI2slSbZlYgYPtd1fa/pJ0iEAQ7ojlAE9l/UHdO27c1rgheun6QrqzMPgIzU287RX9sNy5FW/mEQlujD2p6pTjDu1VSlIeifOddwLUuBw1j/WcnG/Q/o6R1MF8aqz2bCLhGVeZWA19d+Abc7EC346ZT7OtOmBVhUX4OtMwbOVlk+13VzGF+mYacQz0EIdH4tCIwo9naWzWyT05mjJsf3r1F6LiqII65MA057Yuw6iHWoczqPbPqSymN6K5PL/WfB5U1sT1nNqN5WV0U5GThSLXiJCcQzuzRAs0Lc/GLYR0j2mqfa6ME1AC2I1ZlU/7giKjOX6teA4YoKONXqV3TAuzYfxTVtRxaGfo0fZIVgVDn5KhL4USVkrSjpDAP//fTvnjuuoG2bbJtNlBpeWyO1YSPpp/dcSqe+qlSD0GriWw2AZEpJMhy8LZyElgQqxmNmV2ZEbwWvjfPaI1PtbrTGlY1eu1ernn10qIoH+UPnoXjWkVGlBtDln8eZ71nO7ICiKmsZLZJgZh3hlvNGbRrsgMUNqnIDgT091tTFnFmsbZFFIFQ5zRpmIvxl/XFf68flJD+/nhhr7AiMEYuHVCKFkXe0xXnqeUfq6rDC4l0bVmEnwxQnKBPX9EPc+8/09buHTKB2Q/ITiZzNxXgrGiYl+3KNNShqXWUil30inmibIwUMu8lcVPYSCNMabjcRwCEalskmqxAAK+6qNpmfs3m78LZPVtGPa/cS1sRhPsIcrMC+vutvc/iIxDqFZf0L0WLKCYbL1w7pAyTf3uKc8MoFJl+0KGYfvspWF52FloaXkNhfjZyswJ4YFQP1/d1MkDoPgIPv/8nLz0eJ3duhssHxZ7kzcpJXlKUFzGQmc1io45vjU4lihK5sG9ksR07xWf01Ti9J8ZZt5OPZdKvhkQ9b4fbjNmf3XlqmI/F7Mh24xzX3sPsAIW1z80O6K9PT90uPgJB8IdJvzo5YrNPXk7k5i4ncrICWP34ufpzijKdV37g7DxQuLANm7vQKqM9flFvvDFvk5PYlliZIGbec3rE+3Fc6yIs2bJff/7Pq0LO5ECA0KN1I6zcfsDxfq/ccCIufXa2K9mM9vtopvxzerUMKwjkhWy73YkmWhcXoHVx6Ltjl/jw8kH2mxGzbHwEudkBQ5oU5b/4CATBJTcM64RuLQtxft82rtoP6NAErUzVrhqpUTlGB7JXXG2ecjhfrDpENVOVtePZupd4omuqayM1QUFuVoTi+vXwLh6rblnLZKyy5phiwtggSuN49qvEGqIaIY763I0/LDsQLnFedkBfLWjy1MedxYLgC+2bNcC0O07ztHXfTH5OFubcdyaeuNi6hGTcmGZ6dtx/Xg/cd+5xEbZmv5hwdX8M7tTUUhEAkTPwnCxC//aNXfd/xnHWitWoYKwG8D6loYI8WS5XBPEQqyKITHxo38+DqglRM/cETM7i/Jws/fshPgJBSBKtivMTkj8n2k/XadZamJeNX57W2dXu4ZG9WmH63cM9yRbRR+/WeHPMEH0fhRnzwFZmcCpbmbnMR+zyNTmNu8ZcVlkWPoLfntEF950bnqosnh3X57tMEWLGbBoqsNkpDCAiIirbZBq6/7wehk2Y/voIRBEIgk+EQj/twygTmR6iMD/bMVeNW16/aTDuHdk94rhR3oW/PyvMpGOFtjHsxLKm6vU2ZizjisCiiXEDnVERXNyvLYrysnFp/9KE1blY9ehIXB4loV803Igw/jIl/NhYEhXQfARKBy9dPwhdWhRGmIb8yj4qzmJB8Ak3g0Is+wPM9FeTlo3sZV+wfsUfz0HPhz4FoOQp2nWwSunPZr3SpUUhurTogs9W/IjvNu3TjxsH26Zq7eVor+GKE9vjCheDqpdB3Ni2fbMGWPqHcwBEvt+xmnfs8v0UF+RgaJdm+HjpDttr3UQJXdK/FJf0D+Vh0j4Do49A3z+gmYbUKbv4CAQhTbF08Hoco6KZkLq0KMKGcaMwIoofoUFuNjqpq4XyB0e4vu9VJ3UIlyOK3PHMx4ngenOYedOWhlmZPHJBr7Dnp3azr2Wy6KGzLI8blckjF/TEv64aoIfHJgq9hKvBNBS6raYQxEcgCGlJ39LGAIBze0famxu5yBWUaD6+7RQsfcRbgfpL+4fvBfCrzCgRcIeaddXpDnbRUOYVgDlY4L/XD8KGcaMirnv9xpPQuIF1uXRt5/VrN56Ei/uVupIvZijkLCaErwgCPvsIxDQkCD7RqaTQcuABgHduHoLpq3bqNRrqgvycLFuzhx1OG8jC28YkFgDVyapHUkXvyM7k42QJsuv3ZDWVRKP8bBw4Gl42/d/XDMSm3YfDMqBa9fPRb4Zhzrrd0QWwwTi2m/cL6FFDAX9XBKIIBCEJdGjWED8f2tF1e22DW8sE7caOlUSHa17Sry1+NqgdsrMCIHK3ec/ONOQ2xbUd0+44DRt3Hwo7VpiXHZEG2+oux5cW43hDiKsX9BxViJz5RzqLY7qFI6IIBCEN6FxSiPGX9bWNw68rrAbbeAan07qX4KROzcL6cTQN2Qz4OYa8EMO7h/wBXVoUYu3Og/rzto0LsHXfkYjrWxXnR2wstMKvQkDGfrWZf0B3GsuKQBAEICzSJF6evPR4/G7S0oT1B8Q2QIaNa24qCcHeNHRRv7ZYtaMSV53UHqVNQlk/3xozOEwRfDP2DPz5k1X411c/eBcYflTYC70JmunuqJaSOyLFhD+Is1gQhLjwmv9m5j2nY0QP+5WN84rA+nhedhYeHt0LXVoUhflCmhXm6asODW3D3K1ndoVXtEH5uiEdojd0SWglRGisphTZd7gaQOTqR6KGBEFICi9dPwgf/WZY2LExp0ZW4XI7U27frEFYOU4NJ4XSQHWsx+sLAIw1qb0n+Nfuf1mURHIaQzo3C/vvhOZn0IrumEuVio9AEATPvHPzEBw4Uh1XH+bEe3aRULFgNfjn2UQ2Tf7tMCzYuDch9x3ZuxWemrYao2JIJeFld++gsqZY/di5rlOVXDO4Azo2b6gXxTGvCNJuZzERvQjgfAA7mTkiPSER3QPgKoMcPQCUMPMev2QShExjUFlsaZi9EMvYpA1vxmuLC3Jw+4iuOL+PddbYTiWF6OSQ0sIt2ia8eHC7AnJSAqHKdspq45SuIUe3efHj185iP1cELwF4BsDLVieZ+S8A/gIARDQawB2iBAQhffFisbFMUEeE29VNZenO5N8OiyjdaYe+Z8DiXF35CHxTBMw8k4jKXDa/AsAbfskiCIJ/uAz2ySh6t41tT4EZfV+B+i7X21xDRNQAwEgAk6K0GUNE5URUXlFRUXfCCYLgCBttG56vTbAwaUi0t697qyIAQMPcbOV8PY4aGg3gm2hmIWaeyMwDmXlgSYl94ihBENxR1kxJnJaomatXZPUQIrSiinxXnry0D16/6SS0a9oAAaK09BG45XKIWUgQ6pSTOjXDtDtORdcW8Ttfe7RuhD6lxXh4dE/P16bzgiBRO4zzVGdydlZkhwW5WTi5sxZBlIY+AjcQUTGA0wBcnUw5BCET6dayKCH95Odk4UPTPgMnmqs1fAvz6i7pXqpy78jjUJSfg9EONbYJabgiIKI3AAwH0JyItgB4GEAOADDzBLXZxQCmMfMhy04EQaiX3HlWN3Rs3hDnRCmmk6okelJeXJCDsaYym1ZMv2c4Ci024iUCP6OGrnDR5iUoYaaCIGQQ+TlZriqXpSL56g5nv2oz2NG2cYFvfaeCj0AQBCFtePaq/ni7fDO6tUzM5rZUQBSBIAiCB9o0Lqg3G980UiF8VBAEQUgioggEQRAyHFEEgiAIGY4oAkEQhAxHFIEgCEKGI4pAEAQhwxFFIAiCkOGIIhAEQchwyK8amH5BRBUANsZ4eXMAuxIojh+IjPGT6vIBqS9jqssHiIxe6cDMlnn8004RxAMRlTPzwGTLEQ2RMX5SXT4g9WVMdfkAkTGRiGlIEAQhwxFFIAiCkOFkmiKYmGwBXCAyxk+qywekvoypLh8gMiaMjPIRCIIgCJFk2opAEARBMCGKQBAEIcPJGEVARCOJ6HsiWktEY5MkQzsimk5EK4loORHdph5vSkSfEdEa9X8TwzX3qTJ/T0Tn1KGsWUT0HRFNTjUZiagxEb1LRKvU93JIKsmn3vMO9TNeRkRvEFF+smUkoheJaCcRLTMc8ywTEQ0goqXquaeJElOz0Ua+v6if8xIiep+IGidLPjsZDefuJiImoubJlDEmmLne/wHIAvADgE4AcgEsBtAzCXK0BtBffVwEYDWAngD+DGCsenwsgCfVxz1VWfMAdFRfQ1YdyXongNcBTFafp4yMAP4L4Eb1cS6AxikmX1sA6wEUqM/fBvDzZMsI4FQA/QEsMxzzLBOAeQCGACAAUwGc66N8ZwPIVh8/mUz57GRUj7cD8CmUza7NkyljLH+ZsiI4EcBaZl7HzMcAvAngwroWgpm3M/NC9XElgJVQBo0LoQxuUP9fpD6+EMCbzFzFzOsBrIXyWnyFiEoBjALwvOFwSshIRI2g/BhfAABmPsbM+1JFPgPZAAqIKBtAAwDbki0jM88EsMd02JNMRNQaQCNmns3KiPay4ZqEy8fM05i5Rn06B0BpsuSzk1HlrwDuBWCMvkmKjLGQKYqgLYDNhudb1GNJg4jKAPQDMBdAS2beDijKAkALtVmy5P4blC910HAsVWTsBKACwH9U09XzRNQwheQDM28F8BSATQC2A9jPzNNSSUYDXmVqqz42H68LfgFl9gykkHxEdAGArcy82HQqZWR0IlMUgZX9LWlxs0RUCGASgNuZ+UC0phbHfJWbiM4HsJOZF7i9xOKYnzJmQ1maP8vM/QAcgmLSsCMZ72ETKLPBjgDaAGhIRFdHu8TiWLLjuu1kSoqsRPQAgBoAr2mHbOSoU/mIqAGABwA8ZHXaRpaU+7wzRRFsgWLD0yiFslSvc4goB4oSeI2Z31MP/6guF6H+36keT4bcQwFcQEQboJjQziCiV1NIxi0AtjDzXPX5u1AUQ6rIBwAjAKxn5gpmrgbwHoCTU0xGDa8ybUHIPGM87htEdB2A8wFcpZpSUkm+zlAU/mL1N1MKYCERtUohGR3JFEUwH0BXIupIRLkALgfwYV0LoUYGvABgJTOPN5z6EMB16uPrAPzPcPxyIsojoo4AukJxMvkGM9/HzKXMXAblffqSma9OFRmZeQeAzUTUXT10JoAVqSKfyiYAg4mogfqZnwnFH5RKMmp4kkk1H1US0WD1tV1ruCbhENFIAL8DcAEzHzbJnXT5mHkpM7dg5jL1N7MFSkDIjlSR0RXJ9FTX5R+A86BE6fwA4IEkyTAMyhJwCYBF6t95AJoB+ALAGvV/U8M1D6gyf486jiwAMByhqKGUkRHACQDK1ffxAwBNUkk+9Z5/ALAKwDIAr0CJHEmqjADegOKzqIYyYN0Qi0wABqqv6wcAz0DNUOCTfGuh2Nm138uEZMlnJ6Pp/AaoUUPJkjGWP0kxIQiCkOFkimlIEARBsEEUgSAIQoYjikAQBCHDEUUgCIKQ4YgiEARByHBEEQhpAREdVP+XEdGVCe77ftPzbxPZf6Ihop8T0TPJlkOoP4giENKNMgCeFAERZTk0CVMEzHyyR5nSChfvh5BhiCIQ0o1xAE4hokWk5PzPUnPWz1dz1v8SAIhoOCm1H14HsFQ99gERLSClTsAY9dg4KFlCFxHRa+oxbfVBat/L1NzxPzP0/RWFaiK8ZpVPXm3zJBHNI6LVRHSKejxsRk9Ek4louHZv9ZoFRPQ5EZ2o9rNOTW6m0Y6IPiElz/3Dhr6uVu+3iIj+rQ36ar9/JKK5UNIfC0KIZO5mkz/5c/sH4KD6fzjU3c7q8zEAHlQf50HZcdxRbXcIQEdD26bq/wIouzqbGfu2uNelAD6DUs+iJZTUEa3VvvdDyRETADAbwDALmb8C8H/q4/MAfK4+/jmAZwztJgMYrj5mqDtQAbwPYBqAHAB9ASwyXL8dyq5g7bUMBNADwEcActR2/wJwraHfy5L9Ocpfav5le9YcgpBanA2gDxH9RH1eDCWnyzEoeV3WG9reSkQXq4/bqe12R+l7GIA3mLkWSnK2GQAGATig9r0FAIhoERST1SyLPrTEggvUNk4cA/CJ+ngpgCpmriaipabrP2Pm3er931NlrQEwAMB8dYFSgFASuVooyQ4FIQJRBEK6QwB+y8yfhh1UTC2HTM9HABjCzIeJ6CsA+S76tqPK8LgW9r+lKos2NQg3yxrlqGZmLe9LULuemYOkFLnRMOeG0dIb/5eZ77OQ46iq0AQhAvERCOlGJZQynxqfAvgVKem9QUTdSClUY6YYwF5VCRwHYLDhXLV2vYmZAH6m+iFKoFRGS0RW0A0ATiCiABG1Q2zVyM4ipd5wAZTqVt9ASRr3EyJqAej1iDskQF6hniMrAiHdWAKghogWA3gJwN+hmEwWqg7bCliX/fsEwM1EtARKJsg5hnMTASwhooXMfJXh+PtQHKuLocy472XmHaoiiYdvoNQ0XgrFvr8whj5mQclq2gXA68xcDgBE9CCAaUQUgJIh8xYodXQFwRbJPioIgpDhiGlIEAQhwxFFIAiCkOGIIhAEQchwRBEIgiBkOKIIBEEQMhxRBIIgCBmOKAJBEIQM5/8BS1upAIktjhUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Implement softmax.train() by filling in the code to extract a batch of data\n",
    "# and perform the gradient step.\n",
    "import time\n",
    "\n",
    "\n",
    "tic = time.time()\n",
    "loss_hist = softmax.train(X_train, y_train, learning_rate=1e-7,\n",
    "                      num_iters=1500, verbose=True)\n",
    "toc = time.time()\n",
    "print('That took {}s'.format(toc - tic))\n",
    "\n",
    "plt.plot(loss_hist)\n",
    "plt.xlabel('Iteration number')\n",
    "plt.ylabel('Loss value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the performance of the trained softmax classifier on the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: 0.38042857142857145\n",
      "validation accuracy: 0.386\n"
     ]
    }
   ],
   "source": [
    "## Implement softmax.predict() and use it to compute the training and testing error.\n",
    "\n",
    "y_train_pred = softmax.predict(X_train)\n",
    "print('training accuracy: {}'.format(np.mean(np.equal(y_train,y_train_pred), )))\n",
    "y_val_pred = softmax.predict(X_val)\n",
    "print('validation accuracy: {}'.format(np.mean(np.equal(y_val, y_val_pred)), ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize the softmax classifier\n",
    "\n",
    "You may copy and paste your optimization code from the SVM here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.220446049250313e-16"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.finfo(float).eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.001\n",
      "iteration 0 / 1500: loss 2.2835571027153025\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "Learning rate:  0.0001\n",
      "iteration 0 / 1500: loss 2.3741269036684773\n",
      "iteration 100 / 1500: loss 21.566058045533524\n",
      "iteration 200 / 1500: loss 21.58936956028798\n",
      "iteration 300 / 1500: loss 23.002643868236717\n",
      "iteration 400 / 1500: loss 28.245920728779094\n",
      "iteration 500 / 1500: loss 25.92942062535044\n",
      "iteration 600 / 1500: loss 19.075525790192017\n",
      "iteration 700 / 1500: loss 23.70727229305301\n",
      "iteration 800 / 1500: loss 22.159543997997456\n",
      "iteration 900 / 1500: loss 19.685382694143737\n",
      "iteration 1000 / 1500: loss 18.53918056240376\n",
      "iteration 1100 / 1500: loss 37.89698044580087\n",
      "iteration 1200 / 1500: loss 26.190186281951014\n",
      "iteration 1300 / 1500: loss 17.47048053428887\n",
      "iteration 1400 / 1500: loss 34.95236455776998\n",
      "Learning rate:  1e-05\n",
      "iteration 0 / 1500: loss 2.3583198724679124\n",
      "iteration 100 / 1500: loss 3.1441386776978093\n",
      "iteration 200 / 1500: loss 3.480804618374719\n",
      "iteration 300 / 1500: loss 2.542572878400548\n",
      "iteration 400 / 1500: loss 3.147300913233951\n",
      "iteration 500 / 1500: loss 3.426142923236357\n",
      "iteration 600 / 1500: loss 2.5021331818079116\n",
      "iteration 700 / 1500: loss 2.474520066343636\n",
      "iteration 800 / 1500: loss 2.382328730749394\n",
      "iteration 900 / 1500: loss 3.4308575589936847\n",
      "iteration 1000 / 1500: loss 2.081864808305783\n",
      "iteration 1100 / 1500: loss 2.7300271801069598\n",
      "iteration 1200 / 1500: loss 2.541404542039993\n",
      "iteration 1300 / 1500: loss 2.7075820098026564\n",
      "iteration 1400 / 1500: loss 2.551841440383185\n",
      "Learning rate:  1e-06\n",
      "iteration 0 / 1500: loss 2.3416597885450123\n",
      "iteration 100 / 1500: loss 1.8391973732882259\n",
      "iteration 200 / 1500: loss 1.8210858476184548\n",
      "iteration 300 / 1500: loss 1.7657174495064851\n",
      "iteration 400 / 1500: loss 1.7182399427694128\n",
      "iteration 500 / 1500: loss 1.728727141191668\n",
      "iteration 600 / 1500: loss 1.7695810989491774\n",
      "iteration 700 / 1500: loss 1.68452544070862\n",
      "iteration 800 / 1500: loss 1.633023856888361\n",
      "iteration 900 / 1500: loss 1.6483173750169164\n",
      "iteration 1000 / 1500: loss 1.7900163450298086\n",
      "iteration 1100 / 1500: loss 1.8000270831639398\n",
      "iteration 1200 / 1500: loss 1.7500070459884969\n",
      "iteration 1300 / 1500: loss 1.7140488624121668\n",
      "iteration 1400 / 1500: loss 1.84134027719777\n",
      "Learning rate:  1e-07\n",
      "iteration 0 / 1500: loss 2.3513053580344856\n",
      "iteration 100 / 1500: loss 2.068895539834847\n",
      "iteration 200 / 1500: loss 1.9669995283606965\n",
      "iteration 300 / 1500: loss 1.9899268364151783\n",
      "iteration 400 / 1500: loss 1.923360526196894\n",
      "iteration 500 / 1500: loss 1.8847095451002107\n",
      "iteration 600 / 1500: loss 1.8619623955002245\n",
      "iteration 700 / 1500: loss 1.8869851963912563\n",
      "iteration 800 / 1500: loss 1.8467188794182943\n",
      "iteration 900 / 1500: loss 1.8593598344392688\n",
      "iteration 1000 / 1500: loss 1.765959381474745\n",
      "iteration 1100 / 1500: loss 1.840633087672667\n",
      "iteration 1200 / 1500: loss 1.8663659887430333\n",
      "iteration 1300 / 1500: loss 1.8642120445976074\n",
      "iteration 1400 / 1500: loss 1.9952288177197806\n",
      "Learning rate:  1e-08\n",
      "iteration 0 / 1500: loss 2.3440559196467468\n",
      "iteration 100 / 1500: loss 2.2419265217806554\n",
      "iteration 200 / 1500: loss 2.251037140420148\n",
      "iteration 300 / 1500: loss 2.2298865788873776\n",
      "iteration 400 / 1500: loss 2.144391076798619\n",
      "iteration 500 / 1500: loss 2.156646086772012\n",
      "iteration 600 / 1500: loss 2.144818075555676\n",
      "iteration 700 / 1500: loss 2.1036041838125934\n",
      "iteration 800 / 1500: loss 2.0768398300825908\n",
      "iteration 900 / 1500: loss 2.0887748140337137\n",
      "iteration 1000 / 1500: loss 2.0983620723624936\n",
      "iteration 1100 / 1500: loss 2.0631575942638785\n",
      "iteration 1200 / 1500: loss 2.0613572596050656\n",
      "iteration 1300 / 1500: loss 2.101202796663624\n",
      "iteration 1400 / 1500: loss 1.9568810330947901\n",
      "Learning rate:  1e-09\n",
      "iteration 0 / 1500: loss 2.307638514938267\n",
      "iteration 100 / 1500: loss 2.3373064302705306\n",
      "iteration 200 / 1500: loss 2.3230306476333418\n",
      "iteration 300 / 1500: loss 2.309552587840203\n",
      "iteration 400 / 1500: loss 2.306594321755637\n",
      "iteration 500 / 1500: loss 2.3242400819076585\n",
      "iteration 600 / 1500: loss 2.3074075612470675\n",
      "iteration 700 / 1500: loss 2.2834966359908306\n",
      "iteration 800 / 1500: loss 2.2780901677902334\n",
      "iteration 900 / 1500: loss 2.299201740697041\n",
      "iteration 1000 / 1500: loss 2.2503681584965385\n",
      "iteration 1100 / 1500: loss 2.2628266644397157\n",
      "iteration 1200 / 1500: loss 2.2755902076033556\n",
      "iteration 1300 / 1500: loss 2.275820827342614\n",
      "iteration 1400 / 1500: loss 2.2430451942099943\n",
      "Learning rate:  1e-10\n",
      "iteration 0 / 1500: loss 2.3516602062074345\n",
      "iteration 100 / 1500: loss 2.3584117826733135\n",
      "iteration 200 / 1500: loss 2.3317077500657803\n",
      "iteration 300 / 1500: loss 2.3243597562904013\n",
      "iteration 400 / 1500: loss 2.3981623126706966\n",
      "iteration 500 / 1500: loss 2.349918053086092\n",
      "iteration 600 / 1500: loss 2.3234021809969914\n",
      "iteration 700 / 1500: loss 2.289166427605307\n",
      "iteration 800 / 1500: loss 2.319720787510532\n",
      "iteration 900 / 1500: loss 2.3327406191152007\n",
      "iteration 1000 / 1500: loss 2.3269640249527117\n",
      "iteration 1100 / 1500: loss 2.3533928343598776\n",
      "iteration 1200 / 1500: loss 2.3136784358071196\n",
      "iteration 1300 / 1500: loss 2.3435259683116154\n",
      "iteration 1400 / 1500: loss 2.314236459559979\n",
      "lr:  0.001 train acc:  0.10026530612244898 val acc:  0.087\n",
      "lr:  0.0001 train acc:  0.2957959183673469 val acc:  0.299\n",
      "lr:  1e-05 train acc:  0.3962857142857143 val acc:  0.375\n",
      "lr:  1e-06 train acc:  0.4203877551020408 val acc:  0.404\n",
      "lr:  1e-07 train acc:  0.379 val acc:  0.393\n",
      "lr:  1e-08 train acc:  0.2924897959183673 val acc:  0.307\n",
      "lr:  1e-09 train acc:  0.1613265306122449 val acc:  0.152\n",
      "lr:  1e-10 train acc:  0.09683673469387755 val acc:  0.115\n",
      "Best lr:  1e-06\n",
      "Best val acc:  0.404 Best val err:  0.596\n",
      "Best test acc:  0.391 Best test err:  0.609\n"
     ]
    }
   ],
   "source": [
    "# ================================================================ #\n",
    "# YOUR CODE HERE:\n",
    "#   Train the Softmax classifier with different learning rates and \n",
    "#     evaluate on the validation data.\n",
    "#   Report:\n",
    "#     - The best learning rate of the ones you tested.  \n",
    "#     - The best validation accuracy corresponding to the best validation error.\n",
    "#\n",
    "#   Select the SVM that achieved the best validation error and report\n",
    "#     its error rate on the test set.\n",
    "# ================================================================ #\n",
    "from collections import defaultdict\n",
    "lrs = [1e-3,1e-4,1e-5,1e-6,1e-7,1e-8,1e-9,1e-10]\n",
    "lrs_metrics = defaultdict(list) #training error, val error, test_error\n",
    "for lr in lrs:\n",
    "    print(\"Learning rate: \",lr)\n",
    "    softmax = Softmax(dims=[num_classes, num_features])\n",
    "    loss_hist = softmax.train(X_train, y_train, learning_rate=lr,\n",
    "                      num_iters=1500, verbose=True)\n",
    "    y_train_pred = softmax.predict(X_train)\n",
    "    train_acc = np.mean(np.equal(y_train,y_train_pred))\n",
    "    y_val_pred = softmax.predict(X_val)\n",
    "    val_acc = np.mean(np.equal(y_val,y_val_pred))\n",
    "    y_test_pred = softmax.predict(X_test)\n",
    "    test_acc = np.mean(np.equal(y_test,y_test_pred))\n",
    "    \n",
    "    lrs_metrics[lr].append(train_acc)\n",
    "    lrs_metrics[lr].append(val_acc)\n",
    "    lrs_metrics[lr].append(test_acc)\n",
    "                         \n",
    "best_val = 0\n",
    "for k,v in lrs_metrics.items():\n",
    "    print(\"lr: \",k,\"train acc: \",v[0],\"val acc: \",v[1])\n",
    "    if best_val<v[1]:\n",
    "        best_val=v[1]\n",
    "        best_lr = k\n",
    "        best_test = v[2]\n",
    "\n",
    "print(\"Best lr: \",best_lr)\n",
    "print(\"Best val acc: \", best_val, \"Best val err: \", 1-best_val)\n",
    "print(\"Best test acc: \",best_test, \"Best test err: \",1-best_test)\n",
    "\n",
    "# ================================================================ #\n",
    "# END YOUR CODE HERE\n",
    "# ================================================================ #\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
